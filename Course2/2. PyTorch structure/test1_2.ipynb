{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7241dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fd4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0에서 10까지의 원소를 갖는 학습 array 생성\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "# 2x + 1의 값을 갖는 라벨 array 생성\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa551e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d531cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3ad226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "  def __init__(self, inputSize, outputSize):\n",
    "    super(LinearRegression, self).__init__()\n",
    "    self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.linear(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eec8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 및 출력 차원 정의, 하이퍼 파라미터 정의\n",
    "inputDim = 1\n",
    "outputDim = 1\n",
    "learningRate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "# cuda 사용이 가능한 경우, cuda 메모리에 로드\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0500991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function 및 optimizer 정의\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f8774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.0193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 74.019287109375\n",
      "tensor(6.7210, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 6.721015930175781\n",
      "tensor(1.2241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 1.2240792512893677\n",
      "tensor(0.7682, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 0.768163800239563\n",
      "tensor(0.7235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 0.7235128283500671\n",
      "tensor(0.7125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 0.7124910950660706\n",
      "tensor(0.7043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 0.7042949199676514\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 0.6964106559753418\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 0.6886321902275085\n",
      "tensor(0.6809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 0.6809422373771667\n",
      "tensor(0.6733, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 10, loss 0.6733384132385254\n",
      "tensor(0.6658, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 11, loss 0.6658191680908203\n",
      "tensor(0.6584, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 12, loss 0.658383846282959\n",
      "tensor(0.6510, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 13, loss 0.6510319709777832\n",
      "tensor(0.6438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 14, loss 0.6437618732452393\n",
      "tensor(0.6366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 15, loss 0.6365731954574585\n",
      "tensor(0.6295, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 16, loss 0.6294647455215454\n",
      "tensor(0.6224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 17, loss 0.6224355697631836\n",
      "tensor(0.6155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 18, loss 0.6154846549034119\n",
      "tensor(0.6086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 19, loss 0.6086119413375854\n",
      "tensor(0.6018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 20, loss 0.6018157005310059\n",
      "tensor(0.5951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 21, loss 0.5950953960418701\n",
      "tensor(0.5884, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 22, loss 0.5884498953819275\n",
      "tensor(0.5819, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 23, loss 0.5818787813186646\n",
      "tensor(0.5754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 24, loss 0.5753811001777649\n",
      "tensor(0.5690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 25, loss 0.5689557194709778\n",
      "tensor(0.5626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 26, loss 0.5626025795936584\n",
      "tensor(0.5563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 27, loss 0.5563198328018188\n",
      "tensor(0.5501, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 28, loss 0.550107479095459\n",
      "tensor(0.5440, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 29, loss 0.5439643859863281\n",
      "tensor(0.5379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 30, loss 0.5378902554512024\n",
      "tensor(0.5319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 31, loss 0.5318838357925415\n",
      "tensor(0.5259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 32, loss 0.5259442329406738\n",
      "tensor(0.5201, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 33, loss 0.5200710296630859\n",
      "tensor(0.5143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 34, loss 0.5142636299133301\n",
      "tensor(0.5085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 35, loss 0.5085208415985107\n",
      "tensor(0.5028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 36, loss 0.5028424263000488\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 37, loss 0.4972269833087921\n",
      "tensor(0.4917, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 38, loss 0.4916745722293854\n",
      "tensor(0.4862, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 39, loss 0.486184298992157\n",
      "tensor(0.4808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 40, loss 0.48075512051582336\n",
      "tensor(0.4754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 41, loss 0.47538647055625916\n",
      "tensor(0.4701, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 42, loss 0.47007787227630615\n",
      "tensor(0.4648, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 43, loss 0.4648285508155823\n",
      "tensor(0.4596, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 44, loss 0.45963799953460693\n",
      "tensor(0.4545, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 45, loss 0.4545052945613861\n",
      "tensor(0.4494, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 46, loss 0.44943004846572876\n",
      "tensor(0.4444, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 47, loss 0.4444112777709961\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 48, loss 0.43944844603538513\n",
      "tensor(0.4345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 49, loss 0.4345413148403168\n",
      "tensor(0.4297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 50, loss 0.42968887090682983\n",
      "tensor(0.4249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 51, loss 0.4248904883861542\n",
      "tensor(0.4201, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 52, loss 0.42014583945274353\n",
      "tensor(0.4155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 53, loss 0.4154541790485382\n",
      "tensor(0.4108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 54, loss 0.4108148515224457\n",
      "tensor(0.4062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 55, loss 0.4062272012233734\n",
      "tensor(0.4017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 56, loss 0.4016909599304199\n",
      "tensor(0.3972, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 57, loss 0.3972054123878479\n",
      "tensor(0.3928, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 58, loss 0.39276981353759766\n",
      "tensor(0.3884, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 59, loss 0.3883839249610901\n",
      "tensor(0.3840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 60, loss 0.38404688239097595\n",
      "tensor(0.3798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 61, loss 0.37975823879241943\n",
      "tensor(0.3755, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 62, loss 0.3755175471305847\n",
      "tensor(0.3713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 63, loss 0.37132424116134644\n",
      "tensor(0.3672, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 64, loss 0.36717766523361206\n",
      "tensor(0.3631, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 65, loss 0.36307740211486816\n",
      "tensor(0.3590, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 66, loss 0.35902300477027893\n",
      "tensor(0.3550, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 67, loss 0.3550138771533966\n",
      "tensor(0.3510, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 68, loss 0.3510493040084839\n",
      "tensor(0.3471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 69, loss 0.3471294641494751\n",
      "tensor(0.3433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 70, loss 0.34325289726257324\n",
      "tensor(0.3394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 71, loss 0.3394199311733246\n",
      "tensor(0.3356, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 72, loss 0.33562982082366943\n",
      "tensor(0.3319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 73, loss 0.3318817615509033\n",
      "tensor(0.3282, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 74, loss 0.3281759023666382\n",
      "tensor(0.3245, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 75, loss 0.32451102137565613\n",
      "tensor(0.3209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 76, loss 0.3208872973918915\n",
      "tensor(0.3173, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 77, loss 0.3173038959503174\n",
      "tensor(0.3138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 78, loss 0.31376075744628906\n",
      "tensor(0.3103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 79, loss 0.31025704741477966\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 80, loss 0.3067924380302429\n",
      "tensor(0.3034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 81, loss 0.30336645245552063\n",
      "tensor(0.3000, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 82, loss 0.2999788224697113\n",
      "tensor(0.2966, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 83, loss 0.29662904143333435\n",
      "tensor(0.2933, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 84, loss 0.2933167517185211\n",
      "tensor(0.2900, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 85, loss 0.2900412082672119\n",
      "tensor(0.2868, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 86, loss 0.28680238127708435\n",
      "tensor(0.2836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 87, loss 0.2835996747016907\n",
      "tensor(0.2804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 88, loss 0.2804329991340637\n",
      "tensor(0.2773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 89, loss 0.27730122208595276\n",
      "tensor(0.2742, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 90, loss 0.2742045819759369\n",
      "tensor(0.2711, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 91, loss 0.27114272117614746\n",
      "tensor(0.2681, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 92, loss 0.26811474561691284\n",
      "tensor(0.2651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 93, loss 0.26512083411216736\n",
      "tensor(0.2622, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 94, loss 0.2621603012084961\n",
      "tensor(0.2592, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 95, loss 0.25923287868499756\n",
      "tensor(0.2563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 96, loss 0.25633788108825684\n",
      "tensor(0.2535, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 97, loss 0.25347545742988586\n",
      "tensor(0.2506, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 98, loss 0.25064483284950256\n",
      "tensor(0.2478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 99, loss 0.24784602224826813\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  # input과 label을 PyTorch Variable 객체로 변환\n",
    "  if torch.cuda.is_available():\n",
    "    inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "    labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "  else:\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "  # 이전 기록된 gradient를 0으로 초기화\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # model을 통해 input forward propagation 진행\n",
    "  outputs = model(inputs)\n",
    "\n",
    "  # loss 값 계산\n",
    "  loss = criterion(outputs, labels)\n",
    "  print(loss)\n",
    "  # 모든 파라미터에 대해 gradient 계산\n",
    "  loss.backward()\n",
    "\n",
    "  # 파라미터 업데이트\n",
    "  optimizer.step()\n",
    "\n",
    "  print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbebc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07391421]\n",
      " [ 2.207279  ]\n",
      " [ 4.3406434 ]\n",
      " [ 6.474008  ]\n",
      " [ 8.607373  ]\n",
      " [10.740738  ]\n",
      " [12.874103  ]\n",
      " [15.007467  ]\n",
      " [17.140831  ]\n",
      " [19.274197  ]\n",
      " [21.40756   ]]\n"
     ]
    }
   ],
   "source": [
    "# 추론 시 gradient를 계산할 필요가 없으므로 gradient 비활성화\n",
    "with torch.no_grad():\n",
    "  if torch.cuda.is_available():\n",
    "    predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "  else:\n",
    "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "  print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99d1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ce25ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[2.1334]], device='cuda:0')\n",
      "None tensor([0.0739], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# model의 파라미터를 순회하여 name과 parameter 값 출력\n",
    "for p in model.parameters():\n",
    "  if p.requires_grad:\n",
    "    print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb74819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
